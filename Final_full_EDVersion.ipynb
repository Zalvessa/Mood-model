{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zalvessa/Mood-model/blob/main/Final_full_EDVersion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "project_dir = \"ed_project\"\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "\n",
        "# === requirements.txt ===\n",
        "requirements = \"\"\"\\\n",
        "numpy==1.26.4\n",
        "scipy==1.11.4\n",
        "torch==2.1.2\n",
        "torchvision==0.16.2\n",
        "torchaudio==2.1.2\n",
        "transformers==4.35.2\n",
        "peft==0.7.1\n",
        "accelerate==0.24.1\n",
        "scikit-learn\n",
        "textblob\n",
        "vaderSentiment\n",
        "pandas\n",
        "datasets\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(project_dir, \"requirements.txt\"), \"w\") as f:\n",
        "    f.write(requirements.strip())\n",
        "\n",
        "# === train_ed_model.py ===\n",
        "training_script = '''\\\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorWithPadding\n",
        "from datasets import Dataset, Features, Value\n",
        "\n",
        "\n",
        "# Paths\n",
        "PROJECT_DIR = \"/content/ed_project\"\n",
        "\n",
        "# === LOAD DATA ===\n",
        "df = pd.read_csv(f\"{PROJECT_DIR}/synthetic_journal_data.csv\")\n",
        "\n",
        "# === LOAD LABEL SCHEMA ===\n",
        "with open(f\"{PROJECT_DIR}/label_schema.json\", \"r\") as f:\n",
        "    label_schema = json.load(f)\n",
        "\n",
        "\n",
        "label_names = list(label_schema.keys())\n",
        "num_labels = len(label_names)\n",
        "print(\"‚úÖ Loaded label schema with\", num_labels, \"groups\")\n",
        "print(\"‚úÖ Label names:\", label_names)\n",
        "\n",
        "# ‚úÖ Parse labels from single column to list of ints\n",
        "df[\"labels\"] = df[\"labels\"].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
        "print(\"‚úÖ First label vector:\", df[\"labels\"].iloc[0])\n",
        "print(\"‚úÖ Label vector length:\", len(df[\"labels\"].iloc[0]))\n",
        "\n",
        "\n",
        "# ‚úÖ Inspect label distribution across all classes\n",
        "all_labels = np.array(df[\"labels\"].to_list())\n",
        "label_counts = all_labels.sum(axis=0)\n",
        "print(\"‚úÖ Label counts per class:\")\n",
        "for name, count in zip(label_names, label_counts):\n",
        "    print(f\"  {name}: {int(count)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ‚úÖ Optionally sample a smaller dataset\n",
        "df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# ‚úÖ Train/test split\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "# ‚úÖ Keep only necessary columns and reset index\n",
        "train_df = train_df[[\"journal\", \"labels\"]].copy().reset_index(drop=True)\n",
        "val_df = val_df[[\"journal\", \"labels\"]].copy().reset_index(drop=True)\n",
        "\n",
        "# ‚úÖ Cast labels to float32 (matches HuggingFace feature spec)\n",
        "train_df[\"labels\"] = train_df[\"labels\"].apply(lambda x: [np.float32(i) for i in x])\n",
        "val_df[\"labels\"] = val_df[\"labels\"].apply(lambda x: [np.float32(i) for i in x])\n",
        "\n",
        "\n",
        "\n",
        "# ‚úÖ Dataset features\n",
        "features = Features({\n",
        "    \"journal\": Value(\"string\"),\n",
        "    \"labels\": [Value(\"float32\")] * num_labels\n",
        "})\n",
        "\n",
        "# ‚úÖ Convert to HuggingFace Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df[[\"journal\", \"labels\"]], features=features)\n",
        "val_dataset = Dataset.from_pandas(val_df[[\"journal\", \"labels\"]], features=features)\n",
        "\n",
        "# ‚úÖ Tokenizer and model\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=num_labels,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n",
        "\n",
        "# ‚úÖ Tokenize function\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"journal\"], truncation=True, padding=True)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize, batched=True)\n",
        "\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# ‚úÖ Data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "MODEL_OUTPUT_DIR = \"/content/ed_classifier\"\n",
        "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ‚úÖ Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=MODEL_OUTPUT_DIR,  # Save model here\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",   # disables checkpoint saving during training\n",
        "    num_train_epochs=6,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=os.path.join(MODEL_OUTPUT_DIR, \"logs\"),  # Save logs here too\n",
        "    load_best_model_at_end=False,  # must be False if no checkpoints saved\n",
        "    metric_for_best_model=\"f1\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# ‚úÖ Metrics function\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
        "    labels = labels.astype(int)\n",
        "    f1 = f1_score(labels, preds, average=\"micro\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"f1\": f1, \"accuracy\": acc}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ‚úÖ Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# ‚úÖ Train\n",
        "trainer.train()\n",
        "# ‚úÖ Save model manually to ensure weights get written\n",
        "\n",
        "\n",
        "# Re-save the model manually\n",
        "model.save_pretrained(MODEL_OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(MODEL_OUTPUT_DIR)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ‚úÖ Test a forward pass to see raw probabilities\n",
        "with torch.no_grad():\n",
        "    sample = train_dataset[0]\n",
        "    input_ids = sample[\"input_ids\"].unsqueeze(0)\n",
        "    attention_mask = sample[\"attention_mask\"].unsqueeze(0)\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    probs = torch.sigmoid(outputs.logits)\n",
        "    print(\"üîç Sample output probs:\", probs.squeeze().tolist())\n",
        "\n",
        "\n",
        "# ‚úÖ Save\n",
        "trainer.save_model(MODEL_OUTPUT_DIR)\n",
        "\n",
        "print(\"‚úÖ Model saved.\")\n",
        "tokenizer.save_pretrained(MODEL_OUTPUT_DIR)\n",
        "\n",
        "\n",
        "# Save label names into the model directory\n",
        "with open(os.path.join(MODEL_OUTPUT_DIR, \"label_names.json\"), \"w\") as f:\n",
        "    json.dump(label_names, f)\n",
        "\n",
        "print(\"‚úÖ Model trained and saved.\")\n",
        "\n",
        "'''\n",
        "\n",
        "with open(os.path.join(project_dir, \"train_ed_model.py\"), \"w\") as f:\n",
        "    f.write(training_script)\n",
        "\n",
        "print(f\"‚úÖ setup_files.py completed: Files generated in '{project_dir}/'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1L9jKD3I9_r",
        "outputId": "06cd9288-13b3-428f-be9d-d3adf4cd7a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ setup_files.py completed: Files generated in 'ed_project/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers torch torchvision torchaudio numpy scipy peft accelerate\n",
        "!pip install -r ed_project/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WiriCVOSMJoc",
        "outputId": "1c5f915b-0290-4932-a6f5-b49461e4c7df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.35.2\n",
            "Uninstalling transformers-4.35.2:\n",
            "  Successfully uninstalled transformers-4.35.2\n",
            "Found existing installation: torch 2.1.2\n",
            "Uninstalling torch-2.1.2:\n",
            "  Successfully uninstalled torch-2.1.2\n",
            "Found existing installation: torchvision 0.16.2\n",
            "Uninstalling torchvision-0.16.2:\n",
            "  Successfully uninstalled torchvision-0.16.2\n",
            "Found existing installation: torchaudio 2.1.2\n",
            "Uninstalling torchaudio-2.1.2:\n",
            "  Successfully uninstalled torchaudio-2.1.2\n",
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: scipy 1.11.4\n",
            "Uninstalling scipy-1.11.4:\n",
            "  Successfully uninstalled scipy-1.11.4\n",
            "Found existing installation: peft 0.7.1\n",
            "Uninstalling peft-0.7.1:\n",
            "  Successfully uninstalled peft-0.7.1\n",
            "Found existing installation: accelerate 0.24.1\n",
            "Uninstalling accelerate-0.24.1:\n",
            "  Successfully uninstalled accelerate-0.24.1\n",
            "Collecting numpy==1.26.4 (from -r ed_project/requirements.txt (line 1))\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy==1.11.4 (from -r ed_project/requirements.txt (line 2))\n",
            "  Using cached scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting torch==2.1.2 (from -r ed_project/requirements.txt (line 3))\n",
            "  Using cached torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchvision==0.16.2 (from -r ed_project/requirements.txt (line 4))\n",
            "  Using cached torchvision-0.16.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio==2.1.2 (from -r ed_project/requirements.txt (line 5))\n",
            "  Using cached torchaudio-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting transformers==4.35.2 (from -r ed_project/requirements.txt (line 6))\n",
            "  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
            "Collecting peft==0.7.1 (from -r ed_project/requirements.txt (line 7))\n",
            "  Using cached peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting accelerate==0.24.1 (from -r ed_project/requirements.txt (line 8))\n",
            "  Using cached accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r ed_project/requirements.txt (line 9)) (1.6.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (from -r ed_project/requirements.txt (line 10)) (0.19.0)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.11/dist-packages (from -r ed_project/requirements.txt (line 11)) (3.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r ed_project/requirements.txt (line 12)) (2.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from -r ed_project/requirements.txt (line 13)) (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r ed_project/requirements.txt (line 3)) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.2->-r ed_project/requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.2->-r ed_project/requirements.txt (line 4)) (11.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2->-r ed_project/requirements.txt (line 6)) (0.34.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2->-r ed_project/requirements.txt (line 6)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2->-r ed_project/requirements.txt (line 6)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2->-r ed_project/requirements.txt (line 6)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2->-r ed_project/requirements.txt (line 6)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2->-r ed_project/requirements.txt (line 6)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2->-r ed_project/requirements.txt (line 6)) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1->-r ed_project/requirements.txt (line 7)) (5.9.5)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->-r ed_project/requirements.txt (line 3)) (12.5.82)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r ed_project/requirements.txt (line 9)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r ed_project/requirements.txt (line 9)) (3.6.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob->-r ed_project/requirements.txt (line 10)) (3.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r ed_project/requirements.txt (line 12)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r ed_project/requirements.txt (line 12)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r ed_project/requirements.txt (line 12)) (2025.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r ed_project/requirements.txt (line 13)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r ed_project/requirements.txt (line 13)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->-r ed_project/requirements.txt (line 13)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->-r ed_project/requirements.txt (line 13)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ed_project/requirements.txt (line 13)) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->-r ed_project/requirements.txt (line 6)) (1.1.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob->-r ed_project/requirements.txt (line 10)) (8.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r ed_project/requirements.txt (line 12)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2->-r ed_project/requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2->-r ed_project/requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2->-r ed_project/requirements.txt (line 4)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2->-r ed_project/requirements.txt (line 4)) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.2->-r ed_project/requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.2->-r ed_project/requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ed_project/requirements.txt (line 13)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ed_project/requirements.txt (line 13)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ed_project/requirements.txt (line 13)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ed_project/requirements.txt (line 13)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ed_project/requirements.txt (line 13)) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ed_project/requirements.txt (line 13)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ed_project/requirements.txt (line 13)) (1.20.1)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Using cached scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "Using cached torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
            "Using cached torchvision-0.16.2-cp311-cp311-manylinux1_x86_64.whl (6.8 MB)\n",
            "Using cached torchaudio-2.1.2-cp311-cp311-manylinux1_x86_64.whl (3.3 MB)\n",
            "Using cached transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
            "Using cached peft-0.7.1-py3-none-any.whl (168 kB)\n",
            "Using cached accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "Installing collected packages: numpy, scipy, torch, transformers, torchvision, torchaudio, accelerate, peft\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.24.1 numpy-1.26.4 peft-0.7.1 scipy-1.11.4 torch-2.1.2 torchaudio-2.1.2 torchvision-0.16.2 transformers-4.35.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "49b9751304144eb798475836fde3135b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.copy(\"/mnt/data/synthetic_journal_data.csv\", \"synthetic_journal_data.csv\")\n",
        "shutil.copy(\"/mnt/data/label_schema.json\", \"label_schema.json\")\n"
      ],
      "metadata": {
        "id": "WkNwVi8IY3mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python ed_project/train_ed_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsbYlyBDOKDU",
        "outputId": "746a542e-012d-4aa4-d6f0-a01af9db4819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-07 13:48:26.788944: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754574506.853775  119500 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754574506.873142  119500 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754574506.961333  119500 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754574506.961410  119500 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754574506.961509  119500 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754574506.961530  119500 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "‚úÖ Loaded label schema with 34 groups\n",
            "‚úÖ Label names: ['restricting', 'fasting', 'avoidance', 'calorie_counting', 'emotional_overeating', 'impulsive_eating', 'habitual_eating', 'overexposure_triggers', 'compulsive_behavior', 'body_dissatisfaction', 'consistent_eating', 'using_coping_strategies', 'reaching_meal_goals', 'resisting_triggers', 'emotional_regulation', 'expressing_needs', 'tracking_progress', 'seeking_help', 'guilt', 'anxiety', 'sadness', 'shame', 'fear', 'hopelessness', 'self_criticism', 'embarrassment', 'hope', 'confidence', 'calm', 'self_acceptance', 'gratitude', 'relief', 'pride', 'motivation']\n",
            "‚úÖ First label vector: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "‚úÖ Label vector length: 34\n",
            "‚úÖ Label counts per class:\n",
            "  restricting: 136\n",
            "  fasting: 139\n",
            "  avoidance: 147\n",
            "  calorie_counting: 136\n",
            "  emotional_overeating: 147\n",
            "  impulsive_eating: 130\n",
            "  habitual_eating: 161\n",
            "  overexposure_triggers: 140\n",
            "  compulsive_behavior: 136\n",
            "  body_dissatisfaction: 149\n",
            "  consistent_eating: 168\n",
            "  using_coping_strategies: 133\n",
            "  reaching_meal_goals: 141\n",
            "  resisting_triggers: 153\n",
            "  emotional_regulation: 143\n",
            "  expressing_needs: 132\n",
            "  tracking_progress: 129\n",
            "  seeking_help: 136\n",
            "  guilt: 172\n",
            "  anxiety: 139\n",
            "  sadness: 150\n",
            "  shame: 146\n",
            "  fear: 128\n",
            "  hopelessness: 135\n",
            "  self_criticism: 149\n",
            "  embarrassment: 140\n",
            "  hope: 146\n",
            "  confidence: 148\n",
            "  calm: 150\n",
            "  self_acceptance: 129\n",
            "  gratitude: 147\n",
            "  relief: 158\n",
            "  pride: 162\n",
            "  motivation: 142\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map: 100% 1260/1260 [00:00<00:00, 5195.31 examples/s]\n",
            "Map: 100% 140/140 [00:00<00:00, 5621.21 examples/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mspringzola\u001b[0m (\u001b[33mspringzola-scl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250807_134839-93vh4ddo\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgiddy-music-6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/springzola-scl/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/springzola-scl/huggingface/runs/93vh4ddo\u001b[0m\n",
            "  0% 0/948 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 17% 158/948 [08:43<37:30,  2.85s/it]\n",
            "  0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/18 [00:00<00:05,  2.81it/s]\u001b[A\n",
            " 17% 3/18 [00:01<00:07,  2.00it/s]\u001b[A\n",
            " 22% 4/18 [00:02<00:07,  1.75it/s]\u001b[A\n",
            " 28% 5/18 [00:02<00:08,  1.62it/s]\u001b[A\n",
            " 33% 6/18 [00:03<00:08,  1.37it/s]\u001b[A\n",
            " 39% 7/18 [00:04<00:09,  1.16it/s]\u001b[A\n",
            " 44% 8/18 [00:06<00:09,  1.05it/s]\u001b[A\n",
            " 50% 9/18 [00:07<00:08,  1.00it/s]\u001b[A\n",
            " 56% 10/18 [00:07<00:07,  1.09it/s]\u001b[A\n",
            " 61% 11/18 [00:08<00:05,  1.18it/s]\u001b[A\n",
            " 67% 12/18 [00:09<00:04,  1.25it/s]\u001b[A\n",
            " 72% 13/18 [00:09<00:03,  1.31it/s]\u001b[A\n",
            " 78% 14/18 [00:10<00:02,  1.34it/s]\u001b[A\n",
            " 83% 15/18 [00:11<00:02,  1.37it/s]\u001b[A\n",
            " 89% 16/18 [00:12<00:01,  1.39it/s]\u001b[A\n",
            " 94% 17/18 [00:12<00:00,  1.40it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.33257266879081726, 'eval_f1': 0.0, 'eval_accuracy': 0.0, 'eval_runtime': 13.8075, 'eval_samples_per_second': 10.139, 'eval_steps_per_second': 1.304, 'epoch': 1.0}\n",
            " 17% 158/948 [08:57<37:30,  2.85s/it]\n",
            "100% 18/18 [00:13<00:00,  1.64it/s]\u001b[A\n",
            " 33% 316/948 [17:35<30:35,  2.90s/it]\n",
            "  0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/18 [00:00<00:05,  2.86it/s]\u001b[A\n",
            " 17% 3/18 [00:01<00:07,  2.04it/s]\u001b[A\n",
            " 22% 4/18 [00:02<00:07,  1.75it/s]\u001b[A\n",
            " 28% 5/18 [00:02<00:08,  1.62it/s]\u001b[A\n",
            " 33% 6/18 [00:04<00:10,  1.14it/s]\u001b[A\n",
            " 39% 7/18 [00:05<00:12,  1.14s/it]\u001b[A\n",
            " 44% 8/18 [00:07<00:14,  1.42s/it]\u001b[A\n",
            " 50% 9/18 [00:09<00:14,  1.62s/it]\u001b[A\n",
            " 56% 10/18 [00:14<00:19,  2.39s/it]\u001b[A\n",
            " 61% 11/18 [00:15<00:14,  2.10s/it]\u001b[A\n",
            " 67% 12/18 [00:16<00:10,  1.70s/it]\u001b[A\n",
            " 72% 13/18 [00:17<00:07,  1.40s/it]\u001b[A\n",
            " 78% 14/18 [00:17<00:04,  1.19s/it]\u001b[A\n",
            " 83% 15/18 [00:18<00:03,  1.05s/it]\u001b[A\n",
            " 89% 16/18 [00:19<00:01,  1.06it/s]\u001b[A\n",
            " 94% 17/18 [00:19<00:00,  1.14it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.258025586605072, 'eval_f1': 0.0, 'eval_accuracy': 0.0, 'eval_runtime': 20.9809, 'eval_samples_per_second': 6.673, 'eval_steps_per_second': 0.858, 'epoch': 2.0}\n",
            " 33% 316/948 [17:56<30:35,  2.90s/it]\n",
            "100% 18/18 [00:20<00:00,  1.37it/s]\u001b[A\n",
            " 50% 474/948 [26:32<23:22,  2.96s/it]\n",
            "  0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/18 [00:00<00:05,  2.93it/s]\u001b[A\n",
            " 17% 3/18 [00:01<00:07,  2.03it/s]\u001b[A\n",
            " 22% 4/18 [00:02<00:07,  1.75it/s]\u001b[A\n",
            " 28% 5/18 [00:02<00:07,  1.63it/s]\u001b[A\n",
            " 33% 6/18 [00:03<00:07,  1.55it/s]\u001b[A\n",
            " 39% 7/18 [00:04<00:07,  1.50it/s]\u001b[A\n",
            " 44% 8/18 [00:04<00:06,  1.47it/s]\u001b[A\n",
            " 50% 9/18 [00:05<00:06,  1.45it/s]\u001b[A\n",
            " 56% 10/18 [00:06<00:06,  1.30it/s]\u001b[A\n",
            " 61% 11/18 [00:07<00:06,  1.12it/s]\u001b[A\n",
            " 67% 12/18 [00:08<00:05,  1.04it/s]\u001b[A\n",
            " 72% 13/18 [00:09<00:05,  1.00s/it]\u001b[A\n",
            " 78% 14/18 [00:10<00:03,  1.09it/s]\u001b[A\n",
            " 83% 15/18 [00:11<00:02,  1.18it/s]\u001b[A\n",
            " 89% 16/18 [00:12<00:01,  1.24it/s]\u001b[A\n",
            " 94% 17/18 [00:12<00:00,  1.29it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.21082670986652374, 'eval_f1': 0.015717092337917484, 'eval_accuracy': 0.0, 'eval_runtime': 13.8797, 'eval_samples_per_second': 10.087, 'eval_steps_per_second': 1.297, 'epoch': 3.0}\n",
            " 50% 474/948 [26:46<23:22,  2.96s/it]\n",
            "100% 18/18 [00:13<00:00,  1.53it/s]\u001b[A\n",
            "{'loss': 0.3104, 'learning_rate': 9.451476793248946e-06, 'epoch': 3.16}\n",
            " 67% 632/948 [35:38<16:31,  3.14s/it]\n",
            "  0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/18 [00:00<00:05,  2.80it/s]\u001b[A\n",
            " 17% 3/18 [00:01<00:07,  1.97it/s]\u001b[A\n",
            " 22% 4/18 [00:02<00:08,  1.72it/s]\u001b[A\n",
            " 28% 5/18 [00:02<00:08,  1.60it/s]\u001b[A\n",
            " 33% 6/18 [00:03<00:07,  1.53it/s]\u001b[A\n",
            " 39% 7/18 [00:04<00:07,  1.50it/s]\u001b[A\n",
            " 44% 8/18 [00:04<00:06,  1.47it/s]\u001b[A\n",
            " 50% 9/18 [00:05<00:06,  1.45it/s]\u001b[A\n",
            " 56% 10/18 [00:06<00:05,  1.45it/s]\u001b[A\n",
            " 61% 11/18 [00:07<00:04,  1.43it/s]\u001b[A\n",
            " 67% 12/18 [00:07<00:04,  1.42it/s]\u001b[A\n",
            " 72% 13/18 [00:08<00:03,  1.42it/s]\u001b[A\n",
            " 78% 14/18 [00:09<00:03,  1.31it/s]\u001b[A\n",
            " 83% 15/18 [00:10<00:02,  1.12it/s]\u001b[A\n",
            " 89% 16/18 [00:11<00:01,  1.02it/s]\u001b[A\n",
            " 94% 17/18 [00:12<00:01,  1.02s/it]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.1838316172361374, 'eval_f1': 0.061420345489443376, 'eval_accuracy': 0.0, 'eval_runtime': 14.022, 'eval_samples_per_second': 9.984, 'eval_steps_per_second': 1.284, 'epoch': 4.0}\n",
            " 67% 632/948 [35:52<16:31,  3.14s/it]\n",
            "100% 18/18 [00:13<00:00,  1.18it/s]\u001b[A\n",
            " 83% 790/948 [44:42<07:27,  2.83s/it]\n",
            "  0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/18 [00:00<00:05,  2.92it/s]\u001b[A\n",
            " 17% 3/18 [00:01<00:07,  2.02it/s]\u001b[A\n",
            " 22% 4/18 [00:02<00:08,  1.62it/s]\u001b[A\n",
            " 28% 5/18 [00:03<00:10,  1.23it/s]\u001b[A\n",
            " 33% 6/18 [00:04<00:11,  1.07it/s]\u001b[A\n",
            " 39% 7/18 [00:05<00:10,  1.01it/s]\u001b[A\n",
            " 44% 8/18 [00:06<00:09,  1.09it/s]\u001b[A\n",
            " 50% 9/18 [00:07<00:07,  1.18it/s]\u001b[A\n",
            " 56% 10/18 [00:07<00:06,  1.25it/s]\u001b[A\n",
            " 61% 11/18 [00:08<00:05,  1.30it/s]\u001b[A\n",
            " 67% 12/18 [00:09<00:04,  1.34it/s]\u001b[A\n",
            " 72% 13/18 [00:09<00:03,  1.37it/s]\u001b[A\n",
            " 78% 14/18 [00:10<00:02,  1.38it/s]\u001b[A\n",
            " 83% 15/18 [00:11<00:02,  1.39it/s]\u001b[A\n",
            " 89% 16/18 [00:12<00:01,  1.39it/s]\u001b[A\n",
            " 94% 17/18 [00:12<00:00,  1.39it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.16952566802501678, 'eval_f1': 0.18018018018018017, 'eval_accuracy': 0.007142857142857143, 'eval_runtime': 13.8542, 'eval_samples_per_second': 10.105, 'eval_steps_per_second': 1.299, 'epoch': 5.0}\n",
            " 83% 790/948 [44:56<07:27,  2.83s/it]\n",
            "100% 18/18 [00:13<00:00,  1.63it/s]\u001b[A\n",
            "100% 948/948 [53:36<00:00,  2.80s/it]\n",
            "  0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/18 [00:01<00:09,  1.74it/s]\u001b[A\n",
            " 17% 3/18 [00:02<00:12,  1.22it/s]\u001b[A\n",
            " 22% 4/18 [00:03<00:12,  1.08it/s]\u001b[A\n",
            " 28% 5/18 [00:04<00:11,  1.14it/s]\u001b[A\n",
            " 33% 6/18 [00:04<00:09,  1.23it/s]\u001b[A\n",
            " 39% 7/18 [00:05<00:08,  1.27it/s]\u001b[A\n",
            " 44% 8/18 [00:06<00:07,  1.31it/s]\u001b[A\n",
            " 50% 9/18 [00:07<00:06,  1.35it/s]\u001b[A\n",
            " 56% 10/18 [00:07<00:05,  1.37it/s]\u001b[A\n",
            " 61% 11/18 [00:08<00:05,  1.38it/s]\u001b[A\n",
            " 67% 12/18 [00:09<00:04,  1.39it/s]\u001b[A\n",
            " 72% 13/18 [00:09<00:03,  1.39it/s]\u001b[A\n",
            " 78% 14/18 [00:10<00:02,  1.40it/s]\u001b[A\n",
            " 83% 15/18 [00:11<00:02,  1.41it/s]\u001b[A\n",
            " 89% 16/18 [00:11<00:01,  1.41it/s]\u001b[A\n",
            " 94% 17/18 [00:12<00:00,  1.41it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.16492199897766113, 'eval_f1': 0.3250414593698176, 'eval_accuracy': 0.02857142857142857, 'eval_runtime': 13.8898, 'eval_samples_per_second': 10.079, 'eval_steps_per_second': 1.296, 'epoch': 6.0}\n",
            "100% 948/948 [53:50<00:00,  2.80s/it]\n",
            "100% 18/18 [00:13<00:00,  1.66it/s]\u001b[A\n",
            "{'train_runtime': 3231.5342, 'train_samples_per_second': 2.339, 'train_steps_per_second': 0.293, 'train_loss': 0.24867437660442626, 'epoch': 6.0}\n",
            "100% 948/948 [53:50<00:00,  3.41s/it]\n",
            "üîç Sample output probs: [0.13462142646312714, 0.09509802609682083, 0.425761342048645, 0.5232706069946289, 0.05969309061765671, 0.028328634798526764, 0.06747334450483322, 0.05920807644724846, 0.07663577049970627, 0.17609171569347382, 0.06188628822565079, 0.39904192090034485, 0.1066548302769661, 0.10463864356279373, 0.06948342174291611, 0.13393110036849976, 0.08085938543081284, 0.06284494698047638, 0.12063665688037872, 0.4228263795375824, 0.062074802815914154, 0.12691958248615265, 0.0850781500339508, 0.07090987265110016, 0.09731688350439072, 0.06671205908060074, 0.06460531800985336, 0.05769219622015953, 0.05438338965177536, 0.06389880180358887, 0.0840693935751915, 0.10393910109996796, 0.1136423870921135, 0.08548879623413086]\n",
            "‚úÖ Model saved.\n",
            "‚úÖ Model trained and saved.\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mgiddy-music-6\u001b[0m at: \u001b[34mhttps://wandb.ai/springzola-scl/huggingface/runs/93vh4ddo\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250807_134839-93vh4ddo/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# === Colab cell: write predict.py to ed_project ===\n",
        "predict_script = \"\"\"\n",
        "import torch\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "import json\n",
        "import os\n",
        "\n",
        "# === Paths ===\n",
        "MODEL_DIR = \"ed_classifier\"\n",
        "LABEL_FILE = os.path.join(MODEL_DIR, \"label_names.json\")\n",
        "\n",
        "# === Define emotion and behavior groups ===\n",
        "EMOTION_LABELS = {\n",
        "    \"guilt\", \"anxiety\", \"shame\", \"sadness\", \"fear\", \"hopelessness\",\n",
        "    \"self_criticism\", \"embarrassment\", \"hope\", \"confidence\", \"calm\",\n",
        "    \"self_acceptance\", \"gratitude\", \"relief\", \"pride\", \"motivation\"\n",
        "}\n",
        "\n",
        "# Everything else = behavior (fallback)\n",
        "def get_label_type(label):\n",
        "    return \"Emotion\" if label in EMOTION_LABELS else \"Behavior\"\n",
        "\n",
        "# === Load model and tokenizer ===\n",
        "print(\"üîÅ Loading model and tokenizer...\")\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_DIR)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(MODEL_DIR)\n",
        "model.eval()\n",
        "\n",
        "# === Load label names ===\n",
        "with open(LABEL_FILE, \"r\") as f:\n",
        "    label_names = json.load(f)\n",
        "\n",
        "# === Prompt ===\n",
        "print(\"\\\\nüìù Describe how your day went in terms of eating, emotions, and any strategies you used to cope.\\\\n\")\n",
        "prompt=\"\"\n",
        "journal_input = input(prompt)\n",
        "\n",
        "# === Tokenize input ===\n",
        "inputs = tokenizer(journal_input, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "# === Predict ===\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.sigmoid(outputs.logits).squeeze().tolist()\n",
        "\n",
        "# === Get Top 5 predictions ===\n",
        "top_n = 5\n",
        "top_indices = sorted(range(len(probs)), key=lambda i: probs[i], reverse=True)[:top_n]\n",
        "\n",
        "print(\"\\\\nüìä Top 5 Predicted Labels:\")\n",
        "print(\"-\" * 30)\n",
        "for i in top_indices:\n",
        "    label = label_names[i]\n",
        "    label_type = get_label_type(label)\n",
        "    prob = probs[i]\n",
        "    print(f\"{label} ({label_type}): {prob:.3f}\")\n",
        "\"\"\"\n",
        "\n",
        "# Make sure directory exists\n",
        "os.makedirs(\"ed_project\", exist_ok=True)\n",
        "\n",
        "# Write to file\n",
        "with open(\"ed_project/predict.py\", \"w\") as f:\n",
        "    f.write(predict_script)\n",
        "\n",
        "print(\"‚úÖ predict.py created at ed_project/predict.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1e0pov79Xz-",
        "outputId": "84860131-e0c9-401e-9432-0e2ab5a2401c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ predict.py created at ed_project/predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Colab cell: run predict.py ===\n",
        "!python ed_project/predict.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua-bCx_j9oUg",
        "outputId": "f973c745-2de1-47f3-f3fa-1d58e7c178de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Loading model and tokenizer...\n",
            "\n",
            "üìù Describe how your day went in terms of eating, emotions, and any strategies you used to cope.\n",
            "\n",
            "I've been eating three regular meals every day this week. I‚Äôm motivated to keep working on recovery\n",
            "\n",
            "üìä Top 5 Predicted Labels:\n",
            "------------------------------\n",
            "tracking_progress (Behavior): 0.165\n",
            "consistent_eating (Behavior): 0.151\n",
            "pride (Emotion): 0.104\n",
            "motivation (Emotion): 0.083\n",
            "hopelessness (Emotion): 0.060\n"
          ]
        }
      ]
    }
  ]
}